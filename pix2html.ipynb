{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2html.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCZ0cHB696nX"
      },
      "source": [
        "# Dataset import and its preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnvZa1BVV-IW"
      },
      "source": [
        "# imports our files\r\n",
        "%%capture\r\n",
        "import tensorflow as tf\r\n",
        "import time, os\r\n",
        "tf.keras.utils.get_file('pix2html.zip', cache_subdir=os.path.abspath('.'), origin = 'https://raw.githubusercontent.com/psimanaitis/pix2html/main/dataset/pix2html.zip', extract = True)\r\n",
        "tf.keras.utils.get_file('pix2code.zip', cache_subdir=os.path.abspath('.'), origin = 'https://raw.githubusercontent.com/psimanaitis/pix2html/main/dataset/pix2code.zip', extract = True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwBX2D_NtnEY"
      },
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "def get_dataset_vectors(html_entries, tokenizer, input_path):\n",
        "    train_captions = []\n",
        "    img_name_vector = []\n",
        "    for x in range(len(html_entries)):\n",
        "            entry = html_entries[x]\n",
        "            full_input_path =  input_path + entry['id'] + '.jpeg'\n",
        "            img_name_vector.append(full_input_path)\n",
        "            train_captions.append(entry['content'])\n",
        "    \n",
        "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')    \n",
        " \n",
        "    return cap_vector, img_name_vector\n",
        " \n",
        " \n",
        "def load_dataset(dataset_name):\n",
        "    #dataset_name pix2html | pix2code\n",
        "    input_path = dataset_name + '/resized/' \n",
        " \n",
        "    with open(dataset_name + '/tokens.json', 'r') as f:\n",
        "        tokens = json.load(f)\n",
        " \n",
        "    # TODO add start end tokens for pix2code ?\n",
        "    top_k = len(tokens) + 3\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"unknown\", filters='', split=\" \")\n",
        "    tokenizer.fit_on_texts(tokens)\n",
        "    tokenizer.word_index['<pad>'] = 0\n",
        "    tokenizer.index_word[0] = '<pad>'\n",
        " \n",
        "    with open(dataset_name + '/train-html.json', 'r') as f:\n",
        "        train_entries = json.load(f)\n",
        " \n",
        "    with open(dataset_name + '/train-html.json', 'r') as f:\n",
        "        test_entries = json.load(f)\n",
        " \n",
        "    train_sequences, train_images = get_dataset_vectors(train_entries, tokenizer, input_path)\n",
        "    test_sequences, test_images = get_dataset_vectors(test_entries, tokenizer, input_path)   \n",
        " \n",
        " \n",
        "    return train_sequences, train_images, test_sequences, test_images, top_k, tokenizer\n",
        "\n",
        "switcher = { \n",
        "     \"vgg16\": (tf.keras.applications.VGG16(include_top=False,weights='imagenet'), tf.keras.applications.vgg16.preprocess_input, 224, 512),\n",
        "     \"vgg19\": (tf.keras.applications.VGG19(include_top=False,weights='imagenet'), tf.keras.applications.vgg19.preprocess_input, 224, 512),  \n",
        "     \"EfficientNet\": (tf.keras.applications.EfficientNetB3(include_top=False,weights='imagenet'), tf.keras.applications.efficientnet.preprocess_input, 300, 1536),  \n",
        "     \"inception_resnet_v2\": (tf.keras.applications.InceptionResNetV2(include_top=False,weights='imagenet'), tf.keras.applications.inception_resnet_v2.preprocess_input, 299, 1536),\n",
        "     \"resnet\": (tf.keras.applications.ResNet152V2(include_top=False,weights='imagenet'), tf.keras.applications.resnet_v2.preprocess_input, 224, 2048),\n",
        "     \"inception_v3\": (tf.keras.applications.InceptionV3(include_top=False,weights='imagenet'), tf.keras.applications.inception_v3.preprocess_input, 299, 2048),\n",
        "     \"Xception\": (tf.keras.applications.Xception(include_top=False,weights='imagenet'), tf.keras.applications.xception.preprocess_input, 299, 2048),      \n",
        "} \n",
        " \n",
        "def get_load_image(imageModel):\n",
        "    image_model, preprocess_input, dimensions, cnn_features = switcher.get(imageModel)\n",
        "    def load_image(image_path):\n",
        "        img = tf.io.read_file(image_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, (dimensions, dimensions))\n",
        "        img = preprocess_input(img)\n",
        "        return img, image_path\n",
        " \n",
        "    new_input = image_model.input\n",
        "    hidden_layer = image_model.layers[-1].output\n",
        "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        " \n",
        "    return load_image, image_features_extract_model, cnn_features\n",
        " \n",
        "def process_images_to_npy(img_name_vector, image_features_extract_model, load_image):\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(img_name_vector)\n",
        "    image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "    for img, path in image_dataset:\n",
        "        batch_features = image_features_extract_model(img)\n",
        "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        " \n",
        "        for bf, p in zip(batch_features, path):\n",
        "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "            np.save(path_of_feature, bf.numpy())   \n",
        " \n",
        " \n",
        "def map_func(img_name, html):\n",
        "    np_image_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "    return np_image_tensor, html\n",
        " \n",
        " \n",
        "def process_dataset(image, html, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'inputs': image,\n",
        "            'dec_inputs': html[:, :-1]\n",
        "        },\n",
        "        {\n",
        "            'outputs': html[:, 1:]\n",
        "        },\n",
        "    ))  \n",
        "    # TODO sort dataset by sequence length, speeds up training\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        " \n",
        " \n",
        "def get_dataset(imageModel, dataset_name, batch_size):\n",
        "    load_image, image_features_extract_model, cnn_features = get_load_image(imageModel)\n",
        "    train_sequences, train_images, test_sequences, test_images, top_k, tokenizer = load_dataset(dataset_name)\n",
        " \n",
        "    process_images_to_npy(train_images, image_features_extract_model, load_image)\n",
        "    process_images_to_npy(test_images, image_features_extract_model, load_image)\n",
        "    \n",
        "\n",
        "    train_images_tensors = []\n",
        "    for entry in train_images:\n",
        "        train_images_tensors.append( np.load(entry+'.npy'))\n",
        "    \n",
        "    test_images_tensors = []\n",
        "    for entry in test_images:\n",
        "        test_images_tensors.append( np.load(entry+'.npy'))\n",
        "\n",
        "    train_dataset = process_dataset(test_images_tensors, train_sequences, batch_size)\n",
        "    test_dataset = process_dataset(test_images_tensors, test_sequences, batch_size)\n",
        "   \n",
        " \n",
        "    return (train_dataset, test_dataset, top_k, len(train_sequences[0]), tokenizer, cnn_features)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BnS2ydoXIq4"
      },
      "source": [
        "# Model which follow captioning transformer with staked attention architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqqtiAIbXVYT"
      },
      "source": [
        "def create_padding_mask(seq):\r\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\r\n",
        "\r\n",
        "\r\n",
        "def create_look_ahead_mask(size):\r\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\r\n",
        "    return mask\r\n",
        "\r\n",
        "\r\n",
        "def create_masks_decoder(tar):\r\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\r\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\r\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\r\n",
        "    return combined_mask"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS7VsMGSLRFi"
      },
      "source": [
        "%%capture\r\n",
        "!pip install tf-models-official"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMXye6HqcvAE"
      },
      "source": [
        "from official.nlp.transformer.ffn_layer import FeedForwardNetwork\r\n",
        "from official.nlp.modeling.layers.position_embedding import RelativePositionEmbedding\r\n",
        "\r\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\r\n",
        "    inputs = tf.keras.Input((None, d_model), name=\"inputs\")\r\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\r\n",
        "    look_ahead_mask = tf.keras.Input((1, None, None), name=\"look_ahead_mask\")\r\n",
        "\r\n",
        "    attn1 = tf.keras.layers.MultiHeadAttention(value_dim=d_model, key_dim=d_model, num_heads=num_heads, dropout=dropout,\r\n",
        "                                               output_shape=d_model)(inputs, inputs, inputs, look_ahead_mask,\r\n",
        "                                                                     False)  \r\n",
        "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='add_and_norm_1')(attn1 + inputs)\r\n",
        "\r\n",
        "    attn2 = tf.keras.layers.MultiHeadAttention(value_dim=d_model, key_dim=d_model, num_heads=num_heads, dropout=dropout,\r\n",
        "                                               output_shape=d_model)(out1, enc_outputs, enc_outputs, None,\r\n",
        "                                                                     False) \r\n",
        "    out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='add_and_norm_2')(attn2 + out1)\r\n",
        "\r\n",
        "    ffn_output = FeedForwardNetwork(hidden_size=d_model, filter_size=units, relu_dropout=dropout)(out2)\r\n",
        "    out3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='add_and_norm_3')(ffn_output + out2)\r\n",
        "\r\n",
        "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask], outputs=out3, name=name)\r\n",
        "\r\n",
        "\r\n",
        "def decoder(d_model, num_heads, units, target_vocab_size, max_pos_encoding, num_layers=6, dropout=0.1, name=\"decoder\"):\r\n",
        "    inputs = tf.keras.Input((None,), name=\"inputs\")\r\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\r\n",
        "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\r\n",
        "\r\n",
        "    embeddings = tf.keras.layers.Embedding(target_vocab_size, d_model, name='output_embeding')(inputs)\r\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\r\n",
        "    embeddings += RelativePositionEmbedding(max_timescale=max_pos_encoding, hidden_size=d_model)(embeddings)\r\n",
        "\r\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\r\n",
        "\r\n",
        "    for i in range(num_layers):\r\n",
        "        outputs = decoder_layer(units=units, d_model=d_model, num_heads=num_heads, dropout=dropout,\r\n",
        "                                name='decoder_layer_{}'.format(i), )(inputs=[outputs, enc_outputs, look_ahead_mask])\r\n",
        "\r\n",
        "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask], outputs=outputs, name=name)\r\n",
        "\r\n",
        "\r\n",
        "def transformer(vocab_size, units, d_model, num_heads, max_pos_encoding, num_layers, dropout, cnn_features, name=\"transformer\"):\r\n",
        "    inputs = tf.keras.Input((None, cnn_features), name=\"inputs\")\r\n",
        "    dec_inputs = tf.keras.Input((None,), name=\"dec_inputs\")\r\n",
        "\r\n",
        "    dec_mask = tf.keras.layers.Lambda(create_masks_decoder, output_shape=(1, None, None), name='look_ahead_mask')(\r\n",
        "        dec_inputs)\r\n",
        "\r\n",
        "    enc_outputs = tf.keras.layers.Dense(d_model, activation='relu', name='linear_and_relu')(inputs)\r\n",
        "    enc_outputs = tf.keras.layers.Dropout(rate=dropout)(enc_outputs)\r\n",
        "\r\n",
        "    dec_outputs = decoder(target_vocab_size=vocab_size, units=units, d_model=d_model, num_heads=num_heads,\r\n",
        "                          max_pos_encoding=max_pos_encoding, num_layers=num_layers, dropout=dropout)(\r\n",
        "        inputs=[dec_inputs, enc_outputs, dec_mask])\r\n",
        "\r\n",
        "    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax', name=\"outputs\")(dec_outputs)\r\n",
        "\r\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7-iHkySgWxr"
      },
      "source": [
        "\r\n",
        "def custom_accuracy(real, pred):\r\n",
        "  casted = tf.cast(tf.argmax(pred, axis=2), tf.float32)  \r\n",
        "  accuracies = tf.equal(real,  casted)\r\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\r\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\r\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\r\n",
        " \r\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)  \r\n",
        "\r\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\r\n",
        "    loss_ = loss_object(real, pred)\r\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)                                 \r\n",
        "    loss_ *= mask\r\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_XJBCSNkxVK"
      },
      "source": [
        "# Experiments part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DYEh-FgjXWo"
      },
      "source": [
        "from official.nlp.optimization import create_optimizer\r\n",
        "\r\n",
        "\r\n",
        "# get dataset \r\n",
        "batch_size = 32\r\n",
        "cnn_name = 'resnet' # inception_v3 inception_resnet_v2 resnet vgg16 vgg19 EfficientNet Xception' \r\n",
        "dataset_name =  'pix2code' # pix2code pix2html\r\n",
        "\r\n",
        "(dataset, test_dataset, top_k, sequence_length, tokenizerm, cnn_features) = get_dataset(cnn_name, dataset_name, batch_size)\r\n",
        "max_pos_encoding = sequence_length\r\n",
        "vocab_size = top_k + 1  \r\n",
        "\r\n",
        "\r\n",
        "# get optimizer\r\n",
        "epochs = 5\r\n",
        "train_data_size = 1500\r\n",
        "steps_per_epoch = int(train_data_size / batch_size)\r\n",
        "num_train_steps = steps_per_epoch * epochs\r\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\r\n",
        "initial_learning_rate=2e-5\r\n",
        "\r\n",
        "\r\n",
        "optimizer = create_optimizer(init_lr=initial_learning_rate,\r\n",
        "                     num_train_steps=num_train_steps,\r\n",
        "                     num_warmup_steps=warmup_steps,\r\n",
        "                     end_lr=0.0,\r\n",
        "                     optimizer_type='adamw')\r\n",
        "\r\n",
        "# get model\r\n",
        "\r\n",
        "units = 364\r\n",
        "d_model = 182\r\n",
        "num_heads = 8\r\n",
        "num_layers = 6\r\n",
        "dropout = 0.1\r\n",
        "\r\n",
        "model = transformer(d_model = d_model, \r\n",
        "                    units = units,\r\n",
        "                    vocab_size = vocab_size, \r\n",
        "                    max_pos_encoding=max_pos_encoding,\r\n",
        "                    num_layers = num_layers,\r\n",
        "                    dropout = dropout,\r\n",
        "                    num_heads = num_heads,\r\n",
        "                    cnn_features= cnn_features,\r\n",
        "                    name='model')\r\n",
        "\r\n",
        "\r\n",
        "model.compile(optimizer=optimizer, loss=loss_function,  metrics=[custom_accuracy])\r\n",
        "\r\n",
        "history = model.fit(dataset, validation_data=test_dataset, epochs=epochs, verbose=0, batch_size=batch_size)\r\n",
        "\r\n",
        "\r\n",
        "# Variuos utils to visualize and save model\r\n",
        "# model.save('saved_model/my_model')\r\n",
        "# model.sumarry()\r\n",
        "# tf.keras.utils.plot_model(model, to_file='transformer.png', show_shapes=True, expand_nested=True, show_dtype=True, dpi=80)\r\n",
        "# model.save_weights(\"my_weights\")\r\n",
        "# model.save('saved_model/my_model') Does not work because it cannot serialise custom learning rate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}